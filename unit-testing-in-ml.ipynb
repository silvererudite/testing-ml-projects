{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Life cycle of a function\n",
    "Implement, test, accept. If test fail we do bugfixing . We can also get new feature requests and after doing so we test again. Also when someone discovers a previously unseen bug then also after fixing it we need to test again.\n",
    " `Unit test` saves time and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(some_number_str):\n",
    "  return int(some_number_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_string_to_int():\n",
    "  assert convert_to_int(\"20\") == 20\n",
    "test_string_to_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test module\n",
    "A test module containing all of the tests can be run using the command `!pytest test_module_name.py`\n",
    "\n",
    "## Get description of test function\n",
    " `!cat test_module_name.py`\n",
    "## Better Assert statement\n",
    "assert statements can also contain an optional message which makes it easier to understand for anyone to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_string_to_num():\n",
    "  test_argument = \"2081\"\n",
    "  expected = 2081\n",
    "  actual = convert_to_int(test_argument)\n",
    "  # Format the string with the actual return value\n",
    "  message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
    "  # Write the assert statement which prints message on failure\n",
    "  assert  convert_to_int(test_argument) == expected, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with float values\n",
    "Float operation is unexpected so use `pytest.approx(float_value)` to compare floats and wrte assert statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing by raising exceptions\n",
    "`with` statement -> code that is inside this is called `context manager`\n",
    "```\n",
    "with context manager\n",
    "```\n",
    "They perform some function when we enter and exit the function. If the code inside raises an exception, the context manager silences it else it raises anexception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError):\n",
    "  raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Failed",
     "evalue": "DID NOT RAISE <class 'ValueError'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailed\u001b[0m                                    Traceback (most recent call last)",
      "\u001b[1;32mg:\\ml_env\\testing-ml-projects\\unit-testing-in-ml.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/ml_env/testing-ml-projects/unit-testing-in-ml.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m pytest\u001b[39m.\u001b[39mraises(\u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/ml_env/testing-ml-projects/unit-testing-in-ml.ipynb#ch0000013?line=1'>2</a>\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mg:\\ml_env\\env\\lib\\site-packages\\_pytest\\outcomes.py:196\u001b[0m, in \u001b[0;36mfail\u001b[1;34m(reason, pytrace, msg)\u001b[0m\n\u001b[0;32m    194\u001b[0m __tracebackhide__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    195\u001b[0m reason \u001b[39m=\u001b[39m _resolve_msg_to_reason(\u001b[39m\"\u001b[39m\u001b[39mfail\u001b[39m\u001b[39m\"\u001b[39m, reason, msg)\n\u001b[1;32m--> 196\u001b[0m \u001b[39mraise\u001b[39;00m Failed(msg\u001b[39m=\u001b[39mreason, pytrace\u001b[39m=\u001b[39mpytrace)\n",
      "\u001b[1;31mFailed\u001b[0m: DID NOT RAISE <class 'ValueError'>"
     ]
    }
   ],
   "source": [
    "with pytest.raises(ValueError):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also store the exception to check if we are getting the expected error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError) as exception_info:\n",
    "    raise ValueError(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ExceptionInfo' object has no attribute 'contains'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mg:\\ml_env\\testing-ml-projects\\unit-testing-in-ml.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/ml_env/testing-ml-projects/unit-testing-in-ml.ipynb#ch0000016?line=1'>2</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSilence me!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/ml_env/testing-ml-projects/unit-testing-in-ml.ipynb#ch0000016?line=2'>3</a>\u001b[0m \u001b[39m# Check if the raised ValueError contains the correct message\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/ml_env/testing-ml-projects/unit-testing-in-ml.ipynb#ch0000016?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m exc_info\u001b[39m.\u001b[39;49mcontains(\u001b[39m\"\u001b[39m\u001b[39mSilence me!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ExceptionInfo' object has no attribute 'contains'"
     ]
    }
   ],
   "source": [
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "# Check if the raised ValueError contains the correct message\n",
    "assert exc_info.match(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria for testing function\n",
    "Test for \n",
    "1. Bad arguments\n",
    "2. Special arguments\n",
    "3. Normal arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration test\n",
    "Check whether many functions work together as expected instead of independantly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c3a54a9154ab0113f48d48b1eef31653434e6001d3e1c35c5f0767e2012325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
